{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e66061",
   "metadata": {},
   "source": [
    "This is a jupyter demo to show some details in the manuscript \"A Dynamic Pruning Method on Multiple Sparse Structures in Deep Neural Networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c4edec-d657-4a8d-b747-0652351dab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "class DPU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Dynamic Pruning Uint is used to process the dense weights\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask):\n",
    "        # mask indicates which channels in weight tensor are important\n",
    "        # if one channel is important, it will be multiplied by 1, otherwise 0.\n",
    "        out = input * mask\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Pass the gradient directly through\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "class Conv2d_with_DPU(torch.nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Convolutional layer with filter-wise DPU\n",
    "    It has a function named 'calculate_mask' that use l1-norm to evaluate the importance \n",
    "    of dense weights and generate a mask\n",
    "    \"\"\"\n",
    "    def calculate_mask(self, pruning_rate):\n",
    "        # pruning_rate should be between 0 to 1\n",
    "        assert 0 <= pruning_rate < 1\n",
    "        \n",
    "        # the following implementation is output channel pruning and use l1-norm\n",
    "        mask = torch.ones([self.out_channels,])\n",
    "        num_channel_pruned = int(self.out_channels * pruning_rate)\n",
    "        scores = self.weight.abs().sum(dim=(1, 2, 3))\n",
    "        _, index_channel_pruned = torch.topk(scores, num_channel_pruned, largest=False)\n",
    "        mask[index_channel_pruned] = 0\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        # calculate the new weight according mask\n",
    "        weight = self.weight\n",
    "        bias = self.bias\n",
    "        \n",
    "        if hasattr(self, 'mask'):\n",
    "            weight = DPU.apply(self.weight, self.mask.view(-1, 1, 1, 1))\n",
    "            if self.bias is not None:\n",
    "                bias = DPU.apply(self.bias, self.mask)\n",
    "\n",
    "        # Note that the redundant channels always output 0, so the gradient of them\n",
    "        # will disapper when their's outputs go through the ReLU activation function.\n",
    "        # To prevent this problem, we add a tiny positive number to outputs of all\n",
    "        # channels, which has almost no effect on the outputs but can get non-0 gradient\n",
    "        # for the redundant channels.\n",
    "        return self._conv_forward(input, weight, bias) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6afc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1589,  0.3250, -0.2882],\n",
      "          [ 0.2477, -0.0303, -0.2452],\n",
      "          [-0.0641,  0.0733, -0.0732]]],\n",
      "\n",
      "\n",
      "        [[[-0.1772,  0.2492, -0.2567],\n",
      "          [ 0.1221,  0.1858, -0.2877],\n",
      "          [-0.1682, -0.3068, -0.0706]]]], requires_grad=True)\n",
      "tensor([0., 1.])\n",
      "tensor([1.5058, 1.8242], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# create a Conv2d with filter-wise DPU\n",
    "module = Conv2d_with_DPU(in_channels=1,\n",
    "                         out_channels=2,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1,\n",
    "                         bias=False)\n",
    "\n",
    "# create a input\n",
    "input = torch.randn([1, 1, 4, 4])\n",
    "\n",
    "print(module.weight)\n",
    "# calculate module's mask\n",
    "module.calculate_mask(pruning_rate=0.5)\n",
    "# print the mask calculated\n",
    "print(module.mask)\n",
    "# check the scores of each output channel\n",
    "print(module.weight.abs().sum(dim=(1, 2, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc593c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1772,  0.2492, -0.2567],\n",
       "          [ 0.1221,  0.1858, -0.2877],\n",
       "          [-0.1682, -0.3068, -0.0706]]]], grad_fn=<DPUBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observing the W', we can see one of the output channel of weight has been zeros.\n",
    "DPU.apply(module.weight, module.mask.view(-1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b372e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6681,  2.7298,  4.3273],\n",
      "          [ 2.2807,  3.8396,  4.5354],\n",
      "          [ 2.8895,  4.5643,  4.7680]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9566,  3.4354, -1.4009],\n",
      "          [ 1.1768,  5.1519, -0.2187],\n",
      "          [-0.7392, -1.3219,  1.3150]]]])\n"
     ]
    }
   ],
   "source": [
    "output1 = module(input)\n",
    "# Let the output1 go through the relu function and then deriving it,\n",
    "# we can see that the gradients of the redudant channels are not 0, \n",
    "# which indicates that thest gradients can be updated.\n",
    "output2 = torch.nn.functional.relu(output1)\n",
    "output2.sum().backward()\n",
    "print(module.weight.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee5db050",
   "metadata": {},
   "source": [
    "The following is a comparative experiment to demonstrate that the gradient of the redundant weights in the DPU is not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a3267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "module = Conv2d_with_DPU(in_channels=1,\n",
    "                         out_channels=2,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1,\n",
    "                         bias=False)\n",
    "\n",
    "# create a input\n",
    "input = torch.randn([1, 1, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3824ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_normal_pruning's grad:\n",
      " tensor([[[[ 5.7206,  3.4495, -3.1273],\n",
      "          [ 2.4458,  0.5445, -4.4313],\n",
      "          [-3.3191,  2.3399, -1.3938]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "# normal pruning\n",
    "module_normal_pruning = copy.deepcopy(module)\n",
    "module_normal_pruning = prune.ln_structured(\n",
    "    module=module_normal_pruning,\n",
    "    name='weight',\n",
    "    amount=0.5,\n",
    "    n=1,\n",
    "    dim=0\n",
    "    )\n",
    "output1 = module_normal_pruning(input)\n",
    "output2 = torch.nn.functional.relu(output1)\n",
    "output2.sum().backward()\n",
    "print(\"module_normal_pruning's grad:\\n\",\n",
    "      module_normal_pruning.weight_orig.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e245ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "module_DPU's grad:\n",
      " tensor([[[[ 5.7206,  3.4495, -3.1273],\n",
      "          [ 2.4458,  0.5445, -4.4313],\n",
      "          [-3.3191,  2.3399, -1.3938]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2246,  1.5253, -1.9536],\n",
      "          [ 1.5667,  1.3428, -2.7680],\n",
      "          [-0.7879, -1.7737, -5.8281]]]])\n"
     ]
    }
   ],
   "source": [
    "# Conv with DPU\n",
    "module_DPU = copy.deepcopy(module)\n",
    "module_DPU.calculate_mask(pruning_rate=0.5)\n",
    "output3 = module_DPU(input)\n",
    "output4 = torch.nn.functional.relu(output3)\n",
    "output4.sum().backward()\n",
    "print()\n",
    "print(\"module_DPU's grad:\\n\", module_DPU.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc32f3",
   "metadata": {},
   "source": [
    "we can see that the normal pruning using PyTorch implemention get zero grads of the redundant filter, while the DPU get non-zero grads, and the importance channels' grads are equal between two method. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdb42978022f04be5a7fb4489e42213d0776ac652c164c2743d2cf7f8375ef24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
